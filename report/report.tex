\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Efficient Pipeline Parallelism for Reinforcement Learning\\
}

\author{\IEEEauthorblockN{Faiz Sameer Ahmed}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Jose State University}\\
San Jose, USA \\
faiz.ahmed@sjsu.edu}
\and
\IEEEauthorblockN{Dr. Genya Ishigaki}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{San Jose State University}\\
San Jose, USA \\
genya.ishigaki@sjsu.edu}
}

\maketitle

\begin{abstract}
Reinforcement learning trains agents through interaction with environments, alternating between a rollout phase where experiences are collected and a training phase where policy and value networks are updated. Traditional distributed RL architectures transfer collected experiences to a centralized server that trains the actor-critic network and returns updated parameters. However, when deploying large neural networks in resource-constrained environments that require model parallelism across multiple devices, communication overhead increases substantially. Pipeline parallelism necessitates transferring both forward activations and backward gradients between devices for each training epoch, which can be repeated 10 or more times per batch for sample efficiency. This paper presents a gradient accumulation technique that selectively transmits high-magnitude gradients above a specified percentile threshold during backward propagation. By accumulating gradients over time and only communicating the most significant values, we reduce network transfer by up to 90 percent while maintaining comparable training performance to full gradient communication.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement Learning, Pipeline Parallelism, Gradient Compression, Distributed Training, Model Parallelism
\end{IEEEkeywords}

\section{Introduction}

Reinforcement learning trains intelligent agents through environmental interaction, alternating between rollout phases where experiences are collected and training phases where policies are updated. As neural network architectures grow increasingly complex, computational and memory requirements often exceed single-device capabilities, necessitating distributed training approaches.

Traditional distributed RL systems use centralized architectures where edge devices collect experiences while cloud servers train models. However, when models become too large for a single device's memory, pipeline parallelism becomes essential, partitioning networks into sequential stages across multiple devices. Unlike centralized training where only experiences and parameters are transferred, pipeline parallelism requires continuous exchange of forward activations and backward gradients between devices. This communication occurs repeatedly as we run multiple training passes over the data for sample efficiency, creating substantial overhead in bandwidth-limited environments.

This paper addresses this communication challenge through selective gradient transmission. We present a gradient accumulation technique that identifies and communicates only the most significant gradients based on magnitude percentile thresholds. Section II reviews related work. Section III defines the problem and quantifies communication overhead. Section IV describes our method. Section V presents experimental results demonstrating network transfer reduction of 37.5 percent while maintaining comparable performance. Section VI discusses future work.

\section{Related Work}

\subsection{Distributed Reinforcement Learning}

Early work on distributed RL established foundational architectures for scaling training. Nair et al. \cite{nair2015} introduced massively parallel methods for deep RL using parallel actors generating behavior, parallel learners trained from stored experience, and distributed neural networks, achieving order-of-magnitude speedups on Atari games. Building on this, Espeholt et al. \cite{impala2018} proposed IMPALA, which decouples acting from learning using importance sampling for off-policy corrections, scaling to thousands of machines while achieving state-of-the-art results on multi-task benchmarks. Horgan et al. \cite{apex2018} introduced Ape-X, separating data collection from learning through distributed prioritized experience replay, where multiple actors generate experience stored in a shared replay buffer accessed by learners.

\subsection{Communication-Efficient Distributed Training}

Reducing communication overhead in distributed training has been extensively studied. Chen et al. \cite{chen2018} developed communication-efficient policy gradient methods specifically for distributed RL, proposing gradient compression techniques that reduce communication costs while maintaining convergence guarantees. In the supervised learning domain, Lin et al. \cite{dgc2018} discovered that 99.9\% of gradient exchange in distributed SGD is redundant, proposing Deep Gradient Compression (DGC) that achieves 270-600x bandwidth reduction through momentum correction and local gradient clipping. Alistarh et al. \cite{qsgd2017} introduced QSGD, a family of gradient quantization schemes providing convergence guarantees for both convex and non-convex optimization while allowing smooth trade-offs between communication bandwidth and convergence time.

\subsection{Pipeline Parallelism for Large Models}

Pipeline parallelism has emerged as a critical technique for training models exceeding single-device memory. Huang et al. \cite{gpipe2019} presented GPipe, which partitions neural networks across multiple accelerators and pipelines mini-batches through stages, introducing micro-batching and gradient accumulation to achieve near-linear speedup while training models 25x larger than single-accelerator capacity. Narayanan et al. \cite{pipedream2019} proposed PipeDream, which automatically partitions DNN models and schedules computation to minimize pipeline bubbles, maintaining multiple parameter versions to enable weight updates without stalling. For transformer models, Shoeybi et al. \cite{megatron2019} developed Megatron-LM with efficient intra-layer model parallelism, achieving 76\% scaling efficiency on 512 GPUs for 8.3 billion parameter models. Rajbhandari et al. \cite{zero2020} introduced ZeRO, eliminating redundant storage of optimizer states, gradients, and parameters across data-parallel processes, enabling training of 170 billion parameter models.

\subsection{Resource-Constrained Environments}

Recent work addresses distributed learning in resource-constrained settings. Lim et al. \cite{lim2020} applied federated learning principles to RL for IoT devices, enabling multiple agents to collaboratively learn optimal control policies while keeping data local, addressing device-specific dynamics variations. Feng et al. \cite{feng2023} designed IoTSL, an efficient split learning system for resource-constrained IoT devices, optimizing the split learning paradigm to handle non-IID data distribution and reduce communication requirements specific to IoT constraints.

Our work differs from prior approaches by specifically addressing the communication bottleneck in pipeline-parallel RL training through selective gradient accumulation. While existing gradient compression techniques focus on compressing all gradients uniformly or through quantization, our approach accumulates gradients over time and transmits only high-magnitude values above configurable percentile thresholds, tailored to the unique characteristics of RL training with repeated epochs.

\section{Problem Statement}

We consider a distributed RL setup where an agent learns to control the CarRacing-v3 environment through interaction. The agent consists of a convolutional neural network (CNN) for feature extraction and an actor-critic network for policy and value estimation. Given resource constraints, we partition this agent across two machines: Machine 0 hosts the CNN and environment interaction, while Machine 1 hosts the actor-critic network.

\subsection{Communication Overhead Analysis}

We compare data transfer requirements across two deployment scenarios over 100 training iterations with batch size 4096 (100 iterations $\times$ 4096 steps per iteration).

\textbf{Cloud Setup:} The local machine transmits raw observations to the cloud for complete training. Each observation is a $4 \times 96 \times 96$ tensor. Total data transfer:
\begin{equation}
100 \times 4096 \times (4 \times 96 \times 96) = 14.7\text{B tensors} \approx 28\text{ GB}
\end{equation}

\textbf{Pipeline-Parallel Setup:} The CNN runs locally, transmitting activations to Machine 1 and receiving gradients. With 10 update epochs per batch and 32 minibatches per epoch (minibatch size 128), we transmit 4096-dimensional activation and gradient vectors. Total data transfer:
\begin{equation}
100 \times 10 \times 32 \times 128 \times 4096 \times 2 = 32.7\text{B tensors} \approx 66\text{ GB}
\end{equation}

The factor of 2 accounts for bidirectional communication (forward activations and backward gradients). While cloud setup requires 28 GB, pipeline parallelism requires 66 GB—a 2.4$\times$ increase. Table \ref{tab:data_transfer} summarizes this comparison.

\begin{table}[htbp]
\caption{Communication Overhead Comparison}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Setup} & \textbf{Data Transferred} & \textbf{Relative Cost} \\
\hline
Cloud Setup & 28 GB & 1.0$\times$ \\
Pipeline-Parallel & 66 GB & 2.4$\times$ \\
\hline
\end{tabular}
\label{tab:data_transfer}
\end{center}
\end{table}

This overhead stems from repeated gradient exchanges during multiple training epochs. Our goal is to reduce backward gradient communication while maintaining training performance, making pipeline parallelism viable for bandwidth-constrained environments.

\section{Method}

\subsection{Experimental Setup}

We train a reinforcement learning agent on the CarRacing-v3 environment from OpenAI Gym, where the goal is to control a car around a racing track by outputting continuous steering, acceleration, and braking actions. Each observation is a $4 \times 96 \times 96$ RGB image stack representing four consecutive frames. The agent employs Proximal Policy Optimization (PPO) with a total training budget of 500,000 timesteps across 100 iterations, where each iteration consists of 4,096 environment steps followed by 10 update epochs with 32 minibatches of size 128.

To simulate pipeline parallelism across bandwidth-constrained devices, we deploy our system using Docker Compose with two containers connected via a bridge network. Machine 0 hosts the environment and CNN feature extractor, while Machine 1 hosts the actor-critic network. Communication between machines uses PyTorch's distributed RPC framework, which transmits tensors over the network and provides a realistic simulation of inter-device bandwidth limitations.

\subsection{Network Architecture and Partitioning}

The agent network consists of two components distributed across machines:

\textbf{CNN Feature Extractor (Machine 0):} Processes raw observations through three convolutional layers: Conv2d(4$\rightarrow$32, kernel=8, stride=4), Conv2d(32$\rightarrow$64, kernel=4, stride=2), and Conv2d(64$\rightarrow$64, kernel=3, stride=1), each followed by ReLU activation. The output is flattened to a 4,096-dimensional feature vector. This compression from $4 \times 96 \times 96 = 36,864$ input dimensions to 4,096 features provides an 9$\times$ reduction, significantly lowering communication overhead during policy rollout compared to transmitting raw observations.

\textbf{Actor-Critic Network (Machine 1):} Takes the 4,096-dimensional features as input. The actor head produces a 3-dimensional continuous action (steering, acceleration, brake) through a two-layer MLP (4096$\rightarrow$512$\rightarrow$3) with Gaussian policy parameterization and tanh squashing. The critic head estimates state values through a parallel MLP (4096$\rightarrow$512$\rightarrow$1). Both networks use orthogonal weight initialization for stable training.

During policy rollout, Machine 0 sends 4,096-dimensional feature vectors to Machine 1 for action selection—a 9$\times$ bandwidth savings compared to sending raw $4 \times 96 \times 96$ observations. However, the training phase requires bidirectional communication: forward activations and backward gradients must be exchanged for each minibatch across 10 epochs, creating the 66 GB overhead identified in Section III.

\subsection{Gradient Accumulation with Percentile-Based Sparsification}

To reduce backward gradient communication, we introduce a selective gradient transmission strategy inspired by Deep Gradient Compression \cite{dgc2018}. Rather than transmitting full gradient tensors from Machine 1 to Machine 0 after each minibatch, we accumulate gradients locally on Machine 1 and transmit only the most significant values.

\textbf{Accumulation Phase:} Machine 1 maintains a global gradient accumulator $\mathbf{G} \in \mathbb{R}^{b \times d}$ initialized to zero, where $b$ is the minibatch size (128) and $d$ is the feature dimension (4,096). After computing the backward pass for minibatch $i$, we accumulate the feature gradients:
\begin{equation}
\mathbf{G} \leftarrow \mathbf{G} + \nabla_{\mathbf{f}_i} \mathcal{L}
\end{equation}
where $\nabla_{\mathbf{f}_i} \mathcal{L}$ represents gradients with respect to CNN features for minibatch $i$.

\textbf{Sparsification and Transmission:} After accumulation, we compute the $p$-th percentile threshold on absolute gradient magnitudes:
\begin{equation}
\tau = \text{quantile}(|\mathbf{G}|, p)
\end{equation}
where $p \in [0.90, 0.99]$ is a configurable hyperparameter. We create a binary mask $\mathbf{M}$ and transmit only gradients exceeding the threshold:
\begin{equation}
\mathbf{M}_{ij} = \begin{cases}
1 & \text{if } |\mathbf{G}_{ij}| \geq \tau \\
0 & \text{otherwise}
\end{cases}
\end{equation}

The sparse gradient representation consists of three components: (1) indices of non-zero elements stored as int16 coordinates, (2) corresponding gradient values in bfloat16, and (3) tensor shape metadata. For a 90th percentile threshold, we transmit only 10\% of gradient values, achieving a theoretical 10$\times$ compression. After transmission, selected gradients are zeroed in the accumulator:
\begin{equation}
\mathbf{G}_{ij} \leftarrow \begin{cases}
0 & \text{if } \mathbf{M}_{ij} = 1 \\
\mathbf{G}_{ij} & \text{otherwise}
\end{cases}
\end{equation}

\textbf{Reconstruction on Machine 0:} Machine 0 reconstructs the sparse gradient tensor from received indices and values, then applies it to update CNN parameters through standard backpropagation with gradient clipping (max norm 0.5) and Adam optimization (learning rate $5 \times 10^{-5}$).

\textbf{Warm Start:} To ensure stable initial training, we apply full gradient communication for the first 30,000 timesteps before enabling gradient accumulation. This allows the network to establish reasonable initial weights before introducing compression.

The key advantage of this approach for RL is that repeated training epochs naturally generate multiple gradient signals for similar data, making accumulation particularly effective. By deferring transmission until gradients become statistically significant, we reduce communication while preserving the most informative updates for the feature extractor.

\section{Evaluation Results}

We evaluate our gradient accumulation approach across three configurations over 100 training iterations (409,600 timesteps) on CarRacing-v3: (1) baseline pipeline parallelism without compression, (2) 90th percentile sparsification, and (3) 99th percentile sparsification. Episodic returns are smoothed using exponential moving average (EMA) with decay 0.9.

Figure \ref{fig:returns} shows training performance across all configurations. The vertical dashed line marks the warm start boundary at 30,000 steps, after which compression activates. During warm start, all methods exhibit identical performance as expected. Post warm start, the baseline continues steady improvement, reaching a final smoothed return of 546. The 90th percentile configuration maintains comparable learning dynamics, achieving 257 (47\% of baseline). In contrast, the 99th percentile configuration severely impedes convergence, reaching only 30, demonstrating that excessive sparsification discards critical gradient information.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/episodic_returns.pdf}}
\caption{Training performance comparison. EMA-smoothed episodic returns show 90th percentile sparsification maintains reasonable learning dynamics, achieving 47\% of baseline performance. The dashed vertical line indicates warm start completion at 30,000 steps.}
\label{fig:returns}
\end{figure}

Figure \ref{fig:network} presents cumulative data transfer throughout training. The baseline transmits 142.55 GB total, while the 90th percentile configuration achieves 37.5\% reduction (89.13 GB)—equivalent to saving 53.42 GB. Both compressed variants track the baseline during warm start, then diverge as compression activates. The 90th percentile configuration strikes a favorable balance: maintaining reasonable learning performance while achieving substantial bandwidth savings, making it particularly attractive for bandwidth-constrained deployments such as edge computing or federated learning scenarios.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\columnwidth]{figures/network_transfer.pdf}}
\caption{Cumulative network transfer comparison. Gradient accumulation with 90th percentile threshold reduces data transfer by 37.5\%, saving 53.42 GB over 400,000 training steps while maintaining 47\% of baseline learning performance.}
\label{fig:network}
\end{figure}

Our results demonstrate that percentile-based gradient accumulation effectively reduces communication overhead in pipeline-parallel RL training. The 90th percentile threshold provides a practical operating point where bandwidth savings justify the moderate performance trade-off. Unlike supervised learning where gradients exhibit higher redundancy, RL gradients carry environment-specific signals that are more sensitive to lossy compression, suggesting future work on adaptive thresholds that adjust based on training progress.

\section{Future Work (CS298 Plan)}

While this work focused on reducing backward gradient communication, several promising directions remain for further investigation in CS298.

\textbf{Forward Activation Compression:} Our current approach compresses only backward gradients, yet forward activations contribute equally to communication overhead. Applying similar percentile-based sparsification to the 4,096-dimensional feature vectors transmitted during policy rollout could yield additional bandwidth savings. However, unlike gradients which can be accumulated over multiple minibatches, forward activations must be transmitted immediately for action selection, presenting unique challenges for sparsification strategies that maintain policy quality.

\textbf{Autoencoder-Based Compression:} Lightweight autoencoders could learn compressed representations of activations and gradients. Machine 1 would train an encoder compressing feature vectors or gradients into lower-dimensional latent codes, transmitting only these codes and a decoder to Machine 0. This could achieve superior compression ratios while better preserving information about complex gradient distributions.

\textbf{Asynchronous Training:} Our current synchronous pipeline requires Machine 0 to wait for gradient updates from Machine 1 before proceeding to the next minibatch. Implementing asynchronous training—where Machine 0 continues forward passes while Machine 1 processes gradients from previous batches—could improve training throughput and better utilize both machines. This would require careful handling of gradient staleness and parameter versioning, potentially drawing inspiration from IMPALA's importance sampling corrections to maintain training stability despite asynchronous updates.

These extensions would provide a comprehensive framework for communication-efficient pipeline-parallel RL training, addressing both forward and backward data transfer while exploring alternative compression paradigms and training synchronization strategies.

\begin{thebibliography}{00}
\bibitem{nair2015} A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. De Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, et al., ``Massively parallel methods for deep reinforcement learning,'' in \textit{ICML Deep Learning Workshop}, 2015.

\bibitem{impala2018} L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al., ``IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures,'' in \textit{Proc. ICML}, 2018, pp. 1407--1416.

\bibitem{apex2018} D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. Van Hasselt, and D. Silver, ``Distributed prioritized experience replay,'' in \textit{Proc. ICLR}, 2018.

\bibitem{chen2018} W. Chen, S. Wang, Y. Hong, and G. B. Giannakis, ``Communication-efficient policy gradient methods for distributed reinforcement learning,'' \textit{IEEE Trans. Signal Process.}, vol. 69, pp. 2629--2641, 2021.

\bibitem{dgc2018} Y. Lin, S. Han, H. Mao, Y. Wang, and W. J. Dally, ``Deep gradient compression: Reducing the communication bandwidth for distributed training,'' in \textit{Proc. ICLR}, 2018.

\bibitem{qsgd2017} D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, ``QSGD: Communication-efficient SGD via gradient quantization and encoding,'' in \textit{Proc. NeurIPS}, 2017, pp. 1709--1720.

\bibitem{gpipe2019} Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, ``GPipe: Efficient training of giant neural networks using pipeline parallelism,'' in \textit{Proc. NeurIPS}, 2019, pp. 103--112.

\bibitem{pipedream2019} D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia, ``PipeDream: Generalized pipeline parallelism for DNN training,'' in \textit{Proc. ACM SOSP}, 2019, pp. 1--15.

\bibitem{megatron2019} M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, ``Megatron-LM: Training multi-billion parameter language models using model parallelism,'' arXiv preprint arXiv:1909.08053, 2019.

\bibitem{zero2020} S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, ``ZeRO: Memory optimizations toward training trillion parameter models,'' in \textit{Proc. IEEE SC}, 2020, pp. 1--16.

\bibitem{lim2020} H. Lim, Y. J. Kim, and M. L. Sichitiu, ``Federated reinforcement learning for training control policies on multiple IoT devices,'' \textit{Sensors}, vol. 20, no. 5, p. 1359, 2020.

\bibitem{feng2023} X. Feng, C. Luo, J. Chen, Y. Huang, J. Zhang, W. Xu, J. Li, and V. C. M. Leung, ``IoTSL: Toward efficient distributed learning for resource-constrained Internet of Things,'' \textit{IEEE Internet Things J.}, vol. 10, no. 11, pp. 9892--9905, 2023.
\end{thebibliography}
\end{document}